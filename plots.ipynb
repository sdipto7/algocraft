{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cffcede",
   "metadata": {},
   "source": [
    "## Generate relevant plots from the fix reports after running the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88178c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_report(path: Path) -> dict:\n",
    "    metrics = {}\n",
    "    num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n",
    "    with path.open(encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            label, _ = line.split(\":\", 1)\n",
    "            label = label.strip()\n",
    "            m = num_pat.search(line)\n",
    "            if m:\n",
    "                metrics[label] = float(m.group())\n",
    "\n",
    "    required = [\n",
    "        \"Total Instances\", \n",
    "        \"Total Correct\",\n",
    "        \"Total Runtime Failed\", \n",
    "        \"Total Compilation Failed\",\n",
    "        \"Total Test Failed\", \n",
    "        \"Total Infinite Loop\",\n",
    "        \"Accuracy\", \n",
    "        \"Runtime Rate\", \n",
    "        \"Compilation Rate\",\n",
    "        \"Test Failed Rate\", \n",
    "        \"Infinite Loop Rate\"\n",
    "    ]\n",
    "\n",
    "    missing = [k for k in required if k not in metrics]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path.name}: missing fields {missing}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbd7bb",
   "metadata": {},
   "source": [
    "## Mapping generated fix report files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    (\"fix_reports/openai_gpt_4o_avatar_compileReport_from_Python_to_Java.txt\", \"Avatar - Python → Java\"),\n",
    "    (\"fix_reports/openai_gpt_4o_codenet_compileReport_from_Python_to_Java.txt\", \"Codenet - Python → Java\"),\n",
    "    (\"fix_reports/openai_gpt_4o_avatar_compileReport_from_Java_to_Python.txt\", \"Avatar - Java → Python\"),\n",
    "    (\"fix_reports/openai_gpt_4o_codenet_compileReport_from_Java_to_Python.txt\", \"Codenet - Java → Python\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0afeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "records, experiments = {}, []\n",
    "for fname, label in files:\n",
    "    records[label] = parse_report(Path(fname))\n",
    "    experiments.append(label)\n",
    "\n",
    "total_instances = [records[l][\"Total Instances\"] for l in experiments]\n",
    "total_correct = [records[l][\"Total Correct\"] for l in experiments]\n",
    "runtime_failed = [records[l][\"Total Runtime Failed\"] for l in experiments]\n",
    "comp_failed = [records[l][\"Total Compilation Failed\"] for l in experiments]\n",
    "test_failed = [records[l][\"Total Test Failed\"]  for l in experiments]\n",
    "infinite_loop = [records[l][\"Total Infinite Loop\"] for l in experiments]\n",
    "\n",
    "accuracy_rate = [records[l][\"Accuracy\"] for l in experiments]\n",
    "runtime_rate = [records[l][\"Runtime Rate\"] for l in experiments]\n",
    "comp_rate = [records[l][\"Compilation Rate\"] for l in experiments]\n",
    "test_rate = [records[l][\"Test Failed Rate\"] for l in experiments]\n",
    "loop_rate = [records[l][\"Infinite Loop Rate\"] for l in experiments]\n",
    "\n",
    "fail_counts = [i - c for i, c in zip(total_instances, total_correct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257a0e3",
   "metadata": {},
   "source": [
    "## Loading experiment data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Total Inst\": total_instances,\n",
    "    \"Correct\": total_correct,\n",
    "    \"Runtime Fail\": runtime_failed,\n",
    "    \"Compile Fail\": comp_failed,\n",
    "    \"Test Fail\": test_failed,\n",
    "    \"Inf Loop\": infinite_loop,\n",
    "    \"Accuracy (%)\": np.round(accuracy_rate, 2),\n",
    "    \"Runtime Rate (%)\": np.round(runtime_rate, 2),\n",
    "    \"Compile Rate (%)\": np.round(comp_rate, 2),\n",
    "    \"Test Rate (%)\": np.round(test_rate, 2),\n",
    "    \"Loop Rate (%)\": np.round(loop_rate, 2)\n",
    "    }, index=experiments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855376f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('figures'):\n",
    "    os.makedirs('figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2eb2c",
   "metadata": {},
   "source": [
    "## Accuracy bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(9, 5))\n",
    "x = np.arange(len(experiments))\n",
    "bars = ax1.bar(x, accuracy_rate, color='steelblue', edgecolor='black', width=0.55)\n",
    "\n",
    "ax1.set_title(\"Accuracy by Dataset\", pad=15, fontsize=13)\n",
    "ax1.set_ylabel(\"Accuracy (%)\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(experiments, rotation=0, ha='center')\n",
    "\n",
    "ax1.set_ylim(0, max(accuracy_rate) + 10)\n",
    "\n",
    "for bar in bars:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2,\n",
    "             bar.get_height() + 1,\n",
    "             f\"{bar.get_height():.1f}%\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig1.savefig(\"figures/accuracy_comparison.pdf\") # save as pdf\n",
    "fig1.savefig(\"figures/accuracy_comparison.jpg\", dpi=300) # save as jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea03ca",
   "metadata": {},
   "source": [
    "## Stacked failure type bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap = [\"Avatar\\nPython → Java\",\n",
    "        \"Codenet\\nPython → Java\",\n",
    "        \"Avatar\\nJava → Python\",\n",
    "        \"Codenet\\nJava → Python\"]\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(9, 5))\n",
    "p1 = ax2.bar(x, runtime_failed, label=\"Runtime Fail\", width=0.55)\n",
    "p2 = ax2.bar(x, comp_failed,   bottom=runtime_failed,\n",
    "             label=\"Compile Fail\", width=0.55)\n",
    "p3 = ax2.bar(x, test_failed,\n",
    "             bottom=np.array(runtime_failed) + comp_failed,\n",
    "             label=\"Test Fail\", width=0.55)\n",
    "p4 = ax2.bar(x, infinite_loop,\n",
    "             bottom=np.array(runtime_failed) + comp_failed + test_failed,\n",
    "             label=\"Infinite Loop\", width=0.55)\n",
    "\n",
    "ax2.set_title(\"Failure - Type Distribution\", pad=15, fontsize=13)\n",
    "ax2.set_ylabel(\"Number of Instances\")\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(wrap, rotation=0, ha='center', fontsize=10)\n",
    "ax2.legend(loc=\"upper left\", bbox_to_anchor=(1.04, 1.0), borderaxespad=0.)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.22)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig2.savefig(\"figures/failure_types_stacked.pdf\") # save as pdf\n",
    "fig2.savefig(\"figures/failure_types_stacked.jpg\", dpi=300) # save as jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796bcbe",
   "metadata": {},
   "source": [
    "## Success vs Failure grouped bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39401d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots(figsize=(9, 5))\n",
    "w = 0.35\n",
    "ax3.bar(x - w/2, total_correct, w, label=\"Correct\", color='seagreen')\n",
    "ax3.bar(x + w/2, fail_counts,   w, label=\"Failed\",  color='salmon')\n",
    "\n",
    "ax3.set_title(\"Success vs Failure\", pad=15, fontsize=13)\n",
    "ax3.set_ylabel(\"Count\")\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(wrap, rotation=0, ha='center', fontsize=10)\n",
    "ax3.legend(loc=\"upper left\", bbox_to_anchor=(1.04, 1.0), borderaxespad=0.)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.22)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig3.savefig(\"figures/success_vs_failure.pdf\") # save as pdf\n",
    "fig3.savefig(\"figures/success_vs_failure.jpg\", dpi=300) # save as jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19e4a5",
   "metadata": {},
   "source": [
    "## Twin donut charts for overall success vs failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781314f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_labels = [\"Python → Java\", \"Java → Python\"]\n",
    "P2J_idx, J2P_idx = [0, 1], [2, 3]\n",
    "\n",
    "def agg(lst, idxs):\n",
    "    return sum(lst[i] for i in idxs)\n",
    "\n",
    "agg_total_instances = [agg(total_instances, P2J_idx), agg(total_instances, J2P_idx)]\n",
    "agg_total_correct = [agg(total_correct, P2J_idx), agg(total_correct, J2P_idx)]\n",
    "agg_fail_counts = [i - c for i, c in zip(agg_total_instances, agg_total_correct)]\n",
    "\n",
    "agg_accuracy_rate = [c / i * 100 for c, i in zip(agg_total_correct, agg_total_instances)]\n",
    "\n",
    "agg_runtime_failed = [agg(runtime_failed, P2J_idx), agg(runtime_failed, J2P_idx)]\n",
    "agg_comp_failed = [agg(comp_failed, P2J_idx), agg(comp_failed, J2P_idx)]\n",
    "agg_test_failed = [agg(test_failed, P2J_idx), agg(test_failed, J2P_idx)]\n",
    "agg_inf_loop = [agg(infinite_loop, P2J_idx), agg(infinite_loop, J2P_idx)]\n",
    "\n",
    "x2 = np.arange(len(agg_labels))\n",
    "\n",
    "figOC, axesOC = plt.subplots(1, 2, figsize=(8, 4), subplot_kw={\"aspect\": \"equal\"})\n",
    "colors_sf = [[\"seagreen\", \"salmon\"], [\"seagreen\", \"salmon\"]]\n",
    "\n",
    "for ax, label, ok, fail, col in zip(\n",
    "        axesOC, agg_labels, agg_total_correct,\n",
    "        agg_fail_counts, colors_sf):\n",
    "\n",
    "    wedges, _ = ax.pie([ok, fail], startangle=90, colors=col, wedgeprops=dict(width=0.4, edgecolor='white'))\n",
    "    ax.set_title(label, fontsize=11)\n",
    "    \n",
    "    ax.text(0, 0, f\"{ok/(ok+fail)*100:.1f}%\\nCorrect\", fontsize=9, ha='center', va='center')\n",
    "\n",
    "figOC.suptitle(\"Overall Success vs Failure\", y=0.95, fontsize=13)\n",
    "plt.tight_layout()\n",
    "\n",
    "figOC.savefig(\"figures/overall_success_donut.pdf\") # save as pdf\n",
    "figOC.savefig(\"figures/overall_success_donut.jpg\", dpi=300) # save as jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
