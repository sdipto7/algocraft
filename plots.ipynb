{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cffcede",
   "metadata": {},
   "source": [
    "## Generate relevant plots from the fix reports after running the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88178c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_report(path: Path) -> dict:\n",
    "    metrics = {}\n",
    "    num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n",
    "    with path.open(encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            label, _ = line.split(\":\", 1)\n",
    "            label = label.strip()\n",
    "            m = num_pat.search(line)\n",
    "            if m:\n",
    "                metrics[label] = float(m.group())\n",
    "\n",
    "    required = [\n",
    "        \"Total Instances\", \n",
    "        \"Total Correct\",\n",
    "        \"Total Runtime Failed\", \n",
    "        \"Total Compilation Failed\",\n",
    "        \"Total Test Failed\", \n",
    "        \"Total Infinite Loop\",\n",
    "        \"Accuracy\", \n",
    "        \"Runtime Rate\", \n",
    "        \"Compilation Rate\",\n",
    "        \"Test Failed Rate\", \n",
    "        \"Infinite Loop Rate\"\n",
    "    ]\n",
    "\n",
    "    missing = [k for k in required if k not in metrics]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path.name}: missing fields {missing}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbd7bb",
   "metadata": {},
   "source": [
    "## Mapping generated fix report files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    (\"fix_reports/openai_gpt_4o_avatar_compileReport_from_Python_to_Java.txt\", \"Avatar - Python → Java\"),\n",
    "    (\"fix_reports/openai_gpt_4o_codenet_compileReport_from_Python_to_Java.txt\", \"Codenet - Python → Java\"),\n",
    "    (\"fix_reports/openai_gpt_4o_avatar_compileReport_from_Java_to_Python.txt\", \"Avatar - Java → Python\"),\n",
    "    (\"fix_reports/openai_gpt_4o_codenet_compileReport_from_Java_to_Python.txt\", \"Codenet - Java → Python\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0afeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "records, experiments = {}, []\n",
    "for fname, label in files:\n",
    "    records[label] = parse_report(Path(fname))\n",
    "    experiments.append(label)\n",
    "\n",
    "total_instances = [records[l][\"Total Instances\"]          for l in experiments]\n",
    "total_correct   = [records[l][\"Total Correct\"]            for l in experiments]\n",
    "runtime_failed  = [records[l][\"Total Runtime Failed\"]     for l in experiments]\n",
    "comp_failed     = [records[l][\"Total Compilation Failed\"] for l in experiments]\n",
    "test_failed     = [records[l][\"Total Test Failed\"]        for l in experiments]\n",
    "infinite_loop   = [records[l][\"Total Infinite Loop\"]      for l in experiments]\n",
    "\n",
    "accuracy_rate   = [records[l][\"Accuracy\"]             for l in experiments]\n",
    "runtime_rate    = [records[l][\"Runtime Rate\"]         for l in experiments]\n",
    "comp_rate       = [records[l][\"Compilation Rate\"]     for l in experiments]\n",
    "test_rate       = [records[l][\"Test Failed Rate\"]     for l in experiments]\n",
    "loop_rate       = [records[l][\"Infinite Loop Rate\"]   for l in experiments]\n",
    "\n",
    "fail_counts     = [i - c for i, c in zip(total_instances, total_correct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257a0e3",
   "metadata": {},
   "source": [
    "## Loading experiment data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Total Inst\": total_instances,\n",
    "    \"Correct\": total_correct,\n",
    "    \"Runtime Fail\": runtime_failed,\n",
    "    \"Compile Fail\": comp_failed,\n",
    "    \"Test Fail\": test_failed,\n",
    "    \"Inf Loop\": infinite_loop,\n",
    "    \"Accuracy (%)\": np.round(accuracy_rate, 2),\n",
    "    \"Runtime Rate (%)\": np.round(runtime_rate, 2),\n",
    "    \"Compile Rate (%)\": np.round(comp_rate, 2),\n",
    "    \"Test Rate (%)\": np.round(test_rate, 2),\n",
    "    \"Loop Rate (%)\": np.round(loop_rate, 2)\n",
    "    }, index=experiments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855376f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('figures'):\n",
    "    os.makedirs('figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2eb2c",
   "metadata": {},
   "source": [
    "## Accuracy bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(9, 5))\n",
    "x = np.arange(len(experiments))\n",
    "bars = ax1.bar(x, accuracy_rate, color='steelblue', edgecolor='black', width=0.55)\n",
    "\n",
    "ax1.set_title(\"Accuracy by Dataset\", pad=15, fontsize=13)\n",
    "ax1.set_ylabel(\"Accuracy (%)\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(experiments, rotation=0, ha='center')\n",
    "\n",
    "ax1.set_ylim(0, max(accuracy_rate) + 10)\n",
    "\n",
    "for bar in bars:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2,\n",
    "             bar.get_height() + 1,\n",
    "             f\"{bar.get_height():.1f}%\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# fig1.savefig(\"figures/accuracy_comparison.pdf\") # save as pdf\n",
    "# fig1.savefig(\"figures/accuracy_comparison.jpg\", dpi=300) # save as jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
